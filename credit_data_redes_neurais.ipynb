{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('credit.pkl', 'rb') as f:\n",
    "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.46663167\n",
      "Iteration 2, loss = 0.46238807\n",
      "Iteration 3, loss = 0.45853585\n",
      "Iteration 4, loss = 0.45485306\n",
      "Iteration 5, loss = 0.45146936\n",
      "Iteration 6, loss = 0.44839230\n",
      "Iteration 7, loss = 0.44552011\n",
      "Iteration 8, loss = 0.44284030\n",
      "Iteration 9, loss = 0.44036831\n",
      "Iteration 10, loss = 0.43802759\n",
      "Iteration 11, loss = 0.43590331\n",
      "Iteration 12, loss = 0.43382484\n",
      "Iteration 13, loss = 0.43201171\n",
      "Iteration 14, loss = 0.43014260\n",
      "Iteration 15, loss = 0.42840220\n",
      "Iteration 16, loss = 0.42686020\n",
      "Iteration 17, loss = 0.42527004\n",
      "Iteration 18, loss = 0.42376066\n",
      "Iteration 19, loss = 0.42232917\n",
      "Iteration 20, loss = 0.42097422\n",
      "Iteration 21, loss = 0.41958920\n",
      "Iteration 22, loss = 0.41831282\n",
      "Iteration 23, loss = 0.41698708\n",
      "Iteration 24, loss = 0.41569878\n",
      "Iteration 25, loss = 0.41447613\n",
      "Iteration 26, loss = 0.41316115\n",
      "Iteration 27, loss = 0.41193237\n",
      "Iteration 28, loss = 0.41069062\n",
      "Iteration 29, loss = 0.40937919\n",
      "Iteration 30, loss = 0.40813193\n",
      "Iteration 31, loss = 0.40680879\n",
      "Iteration 32, loss = 0.40553562\n",
      "Iteration 33, loss = 0.40417447\n",
      "Iteration 34, loss = 0.40277742\n",
      "Iteration 35, loss = 0.40141219\n",
      "Iteration 36, loss = 0.39998038\n",
      "Iteration 37, loss = 0.39848944\n",
      "Iteration 38, loss = 0.39705230\n",
      "Iteration 39, loss = 0.39552862\n",
      "Iteration 40, loss = 0.39398253\n",
      "Iteration 41, loss = 0.39241092\n",
      "Iteration 42, loss = 0.39082237\n",
      "Iteration 43, loss = 0.38923036\n",
      "Iteration 44, loss = 0.38753750\n",
      "Iteration 45, loss = 0.38586097\n",
      "Iteration 46, loss = 0.38414184\n",
      "Iteration 47, loss = 0.38242837\n",
      "Iteration 48, loss = 0.38066173\n",
      "Iteration 49, loss = 0.37889812\n",
      "Iteration 50, loss = 0.37708249\n",
      "Iteration 51, loss = 0.37526965\n",
      "Iteration 52, loss = 0.37344615\n",
      "Iteration 53, loss = 0.37158649\n",
      "Iteration 54, loss = 0.36973281\n",
      "Iteration 55, loss = 0.36787369\n",
      "Iteration 56, loss = 0.36599725\n",
      "Iteration 57, loss = 0.36410592\n",
      "Iteration 58, loss = 0.36220053\n",
      "Iteration 59, loss = 0.36031208\n",
      "Iteration 60, loss = 0.35839907\n",
      "Iteration 61, loss = 0.35649399\n",
      "Iteration 62, loss = 0.35457348\n",
      "Iteration 63, loss = 0.35264686\n",
      "Iteration 64, loss = 0.35071833\n",
      "Iteration 65, loss = 0.34879657\n",
      "Iteration 66, loss = 0.34684551\n",
      "Iteration 67, loss = 0.34490349\n",
      "Iteration 68, loss = 0.34298377\n",
      "Iteration 69, loss = 0.34101546\n",
      "Iteration 70, loss = 0.33906658\n",
      "Iteration 71, loss = 0.33712552\n",
      "Iteration 72, loss = 0.33519893\n",
      "Iteration 73, loss = 0.33323244\n",
      "Iteration 74, loss = 0.33128953\n",
      "Iteration 75, loss = 0.32935511\n",
      "Iteration 76, loss = 0.32736954\n",
      "Iteration 77, loss = 0.32545360\n",
      "Iteration 78, loss = 0.32351447\n",
      "Iteration 79, loss = 0.32157513\n",
      "Iteration 80, loss = 0.31961462\n",
      "Iteration 81, loss = 0.31768089\n",
      "Iteration 82, loss = 0.31568158\n",
      "Iteration 83, loss = 0.31371575\n",
      "Iteration 84, loss = 0.31177996\n",
      "Iteration 85, loss = 0.30976681\n",
      "Iteration 86, loss = 0.30777685\n",
      "Iteration 87, loss = 0.30595513\n",
      "Iteration 88, loss = 0.30418513\n",
      "Iteration 89, loss = 0.30239869\n",
      "Iteration 90, loss = 0.30065545\n",
      "Iteration 91, loss = 0.29893037\n",
      "Iteration 92, loss = 0.29720639\n",
      "Iteration 93, loss = 0.29546612\n",
      "Iteration 94, loss = 0.29378567\n",
      "Iteration 95, loss = 0.29207716\n",
      "Iteration 96, loss = 0.29038906\n",
      "Iteration 97, loss = 0.28870559\n",
      "Iteration 98, loss = 0.28701033\n",
      "Iteration 99, loss = 0.28532097\n",
      "Iteration 100, loss = 0.28364578\n",
      "Iteration 101, loss = 0.28193520\n",
      "Iteration 102, loss = 0.28020811\n",
      "Iteration 103, loss = 0.27840060\n",
      "Iteration 104, loss = 0.27667244\n",
      "Iteration 105, loss = 0.27489531\n",
      "Iteration 106, loss = 0.27309147\n",
      "Iteration 107, loss = 0.27124536\n",
      "Iteration 108, loss = 0.26942576\n",
      "Iteration 109, loss = 0.26761197\n",
      "Iteration 110, loss = 0.26576266\n",
      "Iteration 111, loss = 0.26387921\n",
      "Iteration 112, loss = 0.26203267\n",
      "Iteration 113, loss = 0.26017893\n",
      "Iteration 114, loss = 0.25828109\n",
      "Iteration 115, loss = 0.25645778\n",
      "Iteration 116, loss = 0.25451422\n",
      "Iteration 117, loss = 0.25263255\n",
      "Iteration 118, loss = 0.25072440\n",
      "Iteration 119, loss = 0.24892980\n",
      "Iteration 120, loss = 0.24716473\n",
      "Iteration 121, loss = 0.24546469\n",
      "Iteration 122, loss = 0.24383604\n",
      "Iteration 123, loss = 0.24220396\n",
      "Iteration 124, loss = 0.24060732\n",
      "Iteration 125, loss = 0.23901452\n",
      "Iteration 126, loss = 0.23745026\n",
      "Iteration 127, loss = 0.23592787\n",
      "Iteration 128, loss = 0.23434090\n",
      "Iteration 129, loss = 0.23279030\n",
      "Iteration 130, loss = 0.23131256\n",
      "Iteration 131, loss = 0.22985890\n",
      "Iteration 132, loss = 0.22838296\n",
      "Iteration 133, loss = 0.22708941\n",
      "Iteration 134, loss = 0.22569441\n",
      "Iteration 135, loss = 0.22433120\n",
      "Iteration 136, loss = 0.22305328\n",
      "Iteration 137, loss = 0.22174097\n",
      "Iteration 138, loss = 0.22050932\n",
      "Iteration 139, loss = 0.21927877\n",
      "Iteration 140, loss = 0.21804899\n",
      "Iteration 141, loss = 0.21683204\n",
      "Iteration 142, loss = 0.21561427\n",
      "Iteration 143, loss = 0.21446397\n",
      "Iteration 144, loss = 0.21326985\n",
      "Iteration 145, loss = 0.21208706\n",
      "Iteration 146, loss = 0.21096325\n",
      "Iteration 147, loss = 0.20981897\n",
      "Iteration 148, loss = 0.20869903\n",
      "Iteration 149, loss = 0.20754112\n",
      "Iteration 150, loss = 0.20647064\n",
      "Iteration 151, loss = 0.20536642\n",
      "Iteration 152, loss = 0.20427467\n",
      "Iteration 153, loss = 0.20316032\n",
      "Iteration 154, loss = 0.20213501\n",
      "Iteration 155, loss = 0.20103613\n",
      "Iteration 156, loss = 0.19998834\n",
      "Iteration 157, loss = 0.19892429\n",
      "Iteration 158, loss = 0.19791422\n",
      "Iteration 159, loss = 0.19686970\n",
      "Iteration 160, loss = 0.19589373\n",
      "Iteration 161, loss = 0.19487774\n",
      "Iteration 162, loss = 0.19388058\n",
      "Iteration 163, loss = 0.19288808\n",
      "Iteration 164, loss = 0.19190438\n",
      "Iteration 165, loss = 0.19093704\n",
      "Iteration 166, loss = 0.18997386\n",
      "Iteration 167, loss = 0.18901739\n",
      "Iteration 168, loss = 0.18806623\n",
      "Iteration 169, loss = 0.18712079\n",
      "Iteration 170, loss = 0.18622689\n",
      "Iteration 171, loss = 0.18526858\n",
      "Iteration 172, loss = 0.18433783\n",
      "Iteration 173, loss = 0.18344962\n",
      "Iteration 174, loss = 0.18251741\n",
      "Iteration 175, loss = 0.18163197\n",
      "Iteration 176, loss = 0.18071251\n",
      "Iteration 177, loss = 0.17984836\n",
      "Iteration 178, loss = 0.17897386\n",
      "Iteration 179, loss = 0.17805733\n",
      "Iteration 180, loss = 0.17719460\n",
      "Iteration 181, loss = 0.17634111\n",
      "Iteration 182, loss = 0.17551238\n",
      "Iteration 183, loss = 0.17464589\n",
      "Iteration 184, loss = 0.17377682\n",
      "Iteration 185, loss = 0.17291014\n",
      "Iteration 186, loss = 0.17207583\n",
      "Iteration 187, loss = 0.17127402\n",
      "Iteration 188, loss = 0.17042592\n",
      "Iteration 189, loss = 0.16960552\n",
      "Iteration 190, loss = 0.16878796\n",
      "Iteration 191, loss = 0.16799432\n",
      "Iteration 192, loss = 0.16714848\n",
      "Iteration 193, loss = 0.16634851\n",
      "Iteration 194, loss = 0.16553330\n",
      "Iteration 195, loss = 0.16475057\n",
      "Iteration 196, loss = 0.16396417\n",
      "Iteration 197, loss = 0.16315236\n",
      "Iteration 198, loss = 0.16240326\n",
      "Iteration 199, loss = 0.16160941\n",
      "Iteration 200, loss = 0.16083748\n",
      "Iteration 201, loss = 0.16009564\n",
      "Iteration 202, loss = 0.15930453\n",
      "Iteration 203, loss = 0.15856592\n",
      "Iteration 204, loss = 0.15782343\n",
      "Iteration 205, loss = 0.15704866\n",
      "Iteration 206, loss = 0.15629787\n",
      "Iteration 207, loss = 0.15555956\n",
      "Iteration 208, loss = 0.15484808\n",
      "Iteration 209, loss = 0.15407101\n",
      "Iteration 210, loss = 0.15335736\n",
      "Iteration 211, loss = 0.15264344\n",
      "Iteration 212, loss = 0.15189227\n",
      "Iteration 213, loss = 0.15118235\n",
      "Iteration 214, loss = 0.15045395\n",
      "Iteration 215, loss = 0.14978477\n",
      "Iteration 216, loss = 0.14904979\n",
      "Iteration 217, loss = 0.14834033\n",
      "Iteration 218, loss = 0.14764032\n",
      "Iteration 219, loss = 0.14692721\n",
      "Iteration 220, loss = 0.14623742\n",
      "Iteration 221, loss = 0.14555334\n",
      "Iteration 222, loss = 0.14485738\n",
      "Iteration 223, loss = 0.14415764\n",
      "Iteration 224, loss = 0.14349461\n",
      "Iteration 225, loss = 0.14282346\n",
      "Iteration 226, loss = 0.14212655\n",
      "Iteration 227, loss = 0.14145474\n",
      "Iteration 228, loss = 0.14080552\n",
      "Iteration 229, loss = 0.14014355\n",
      "Iteration 230, loss = 0.13948793\n",
      "Iteration 231, loss = 0.13882311\n",
      "Iteration 232, loss = 0.13817081\n",
      "Iteration 233, loss = 0.13753514\n",
      "Iteration 234, loss = 0.13688359\n",
      "Iteration 235, loss = 0.13624335\n",
      "Iteration 236, loss = 0.13560280\n",
      "Iteration 237, loss = 0.13499023\n",
      "Iteration 238, loss = 0.13435067\n",
      "Iteration 239, loss = 0.13373262\n",
      "Iteration 240, loss = 0.13312429\n",
      "Iteration 241, loss = 0.13249936\n",
      "Iteration 242, loss = 0.13189719\n",
      "Iteration 243, loss = 0.13127889\n",
      "Iteration 244, loss = 0.13067080\n",
      "Iteration 245, loss = 0.13007351\n",
      "Iteration 246, loss = 0.12948817\n",
      "Iteration 247, loss = 0.12887743\n",
      "Iteration 248, loss = 0.12829517\n",
      "Iteration 249, loss = 0.12771431\n",
      "Iteration 250, loss = 0.12712757\n",
      "Iteration 251, loss = 0.12654298\n",
      "Iteration 252, loss = 0.12595525\n",
      "Iteration 253, loss = 0.12538451\n",
      "Iteration 254, loss = 0.12480687\n",
      "Iteration 255, loss = 0.12426451\n",
      "Iteration 256, loss = 0.12366343\n",
      "Iteration 257, loss = 0.12309895\n",
      "Iteration 258, loss = 0.12254233\n",
      "Iteration 259, loss = 0.12197773\n",
      "Iteration 260, loss = 0.12141467\n",
      "Iteration 261, loss = 0.12087023\n",
      "Iteration 262, loss = 0.12035121\n",
      "Iteration 263, loss = 0.11977095\n",
      "Iteration 264, loss = 0.11924699\n",
      "Iteration 265, loss = 0.11867107\n",
      "Iteration 266, loss = 0.11814103\n",
      "Iteration 267, loss = 0.11762427\n",
      "Iteration 268, loss = 0.11708278\n",
      "Iteration 269, loss = 0.11658049\n",
      "Iteration 270, loss = 0.11603777\n",
      "Iteration 271, loss = 0.11551402\n",
      "Iteration 272, loss = 0.11501640\n",
      "Iteration 273, loss = 0.11447419\n",
      "Iteration 274, loss = 0.11397713\n",
      "Iteration 275, loss = 0.11348503\n",
      "Iteration 276, loss = 0.11296947\n",
      "Iteration 277, loss = 0.11244884\n",
      "Iteration 278, loss = 0.11193669\n",
      "Iteration 279, loss = 0.11146339\n",
      "Iteration 280, loss = 0.11094674\n",
      "Iteration 281, loss = 0.11045328\n",
      "Iteration 282, loss = 0.10996825\n",
      "Iteration 283, loss = 0.10947855\n",
      "Iteration 284, loss = 0.10897891\n",
      "Iteration 285, loss = 0.10850424\n",
      "Iteration 286, loss = 0.10801576\n",
      "Iteration 287, loss = 0.10755300\n",
      "Iteration 288, loss = 0.10707067\n",
      "Iteration 289, loss = 0.10659132\n",
      "Iteration 290, loss = 0.10611971\n",
      "Iteration 291, loss = 0.10564301\n",
      "Iteration 292, loss = 0.10519080\n",
      "Iteration 293, loss = 0.10472189\n",
      "Iteration 294, loss = 0.10425599\n",
      "Iteration 295, loss = 0.10378897\n",
      "Iteration 296, loss = 0.10335317\n",
      "Iteration 297, loss = 0.10289476\n",
      "Iteration 298, loss = 0.10243789\n",
      "Iteration 299, loss = 0.10198733\n",
      "Iteration 300, loss = 0.10155469\n",
      "Iteration 301, loss = 0.10109779\n",
      "Iteration 302, loss = 0.10065968\n",
      "Iteration 303, loss = 0.10023519\n",
      "Iteration 304, loss = 0.09978913\n",
      "Iteration 305, loss = 0.09935221\n",
      "Iteration 306, loss = 0.09891054\n",
      "Iteration 307, loss = 0.09847926\n",
      "Iteration 308, loss = 0.09805134\n",
      "Iteration 309, loss = 0.09762075\n",
      "Iteration 310, loss = 0.09718775\n",
      "Iteration 311, loss = 0.09678444\n",
      "Iteration 312, loss = 0.09634724\n",
      "Iteration 313, loss = 0.09594916\n",
      "Iteration 314, loss = 0.09551825\n",
      "Iteration 315, loss = 0.09510968\n",
      "Iteration 316, loss = 0.09470165\n",
      "Iteration 317, loss = 0.09429279\n",
      "Iteration 318, loss = 0.09388648\n",
      "Iteration 319, loss = 0.09349127\n",
      "Iteration 320, loss = 0.09308986\n",
      "Iteration 321, loss = 0.09268402\n",
      "Iteration 322, loss = 0.09229498\n",
      "Iteration 323, loss = 0.09188625\n",
      "Iteration 324, loss = 0.09148507\n",
      "Iteration 325, loss = 0.09111440\n",
      "Iteration 326, loss = 0.09070818\n",
      "Iteration 327, loss = 0.09032910\n",
      "Iteration 328, loss = 0.08993918\n",
      "Iteration 329, loss = 0.08956403\n",
      "Iteration 330, loss = 0.08918020\n",
      "Iteration 331, loss = 0.08879562\n",
      "Iteration 332, loss = 0.08842003\n",
      "Iteration 333, loss = 0.08803463\n",
      "Iteration 334, loss = 0.08767446\n",
      "Iteration 335, loss = 0.08729496\n",
      "Iteration 336, loss = 0.08693327\n",
      "Iteration 337, loss = 0.08654312\n",
      "Iteration 338, loss = 0.08618353\n",
      "Iteration 339, loss = 0.08582664\n",
      "Iteration 340, loss = 0.08549426\n",
      "Iteration 341, loss = 0.08511924\n",
      "Iteration 342, loss = 0.08477800\n",
      "Iteration 343, loss = 0.08443112\n",
      "Iteration 344, loss = 0.08406679\n",
      "Iteration 345, loss = 0.08371016\n",
      "Iteration 346, loss = 0.08338266\n",
      "Iteration 347, loss = 0.08302917\n",
      "Iteration 348, loss = 0.08269199\n",
      "Iteration 349, loss = 0.08234018\n",
      "Iteration 350, loss = 0.08201118\n",
      "Iteration 351, loss = 0.08168012\n",
      "Iteration 352, loss = 0.08134002\n",
      "Iteration 353, loss = 0.08101833\n",
      "Iteration 354, loss = 0.08068885\n",
      "Iteration 355, loss = 0.08037068\n",
      "Iteration 356, loss = 0.08004185\n",
      "Iteration 357, loss = 0.07970822\n",
      "Iteration 358, loss = 0.07938663\n",
      "Iteration 359, loss = 0.07907449\n",
      "Iteration 360, loss = 0.07876336\n",
      "Iteration 361, loss = 0.07843754\n",
      "Iteration 362, loss = 0.07811491\n",
      "Iteration 363, loss = 0.07779807\n",
      "Iteration 364, loss = 0.07749318\n",
      "Iteration 365, loss = 0.07717757\n",
      "Iteration 366, loss = 0.07687875\n",
      "Iteration 367, loss = 0.07655963\n",
      "Iteration 368, loss = 0.07627069\n",
      "Iteration 369, loss = 0.07596021\n",
      "Iteration 370, loss = 0.07564344\n",
      "Iteration 371, loss = 0.07534867\n",
      "Iteration 372, loss = 0.07504170\n",
      "Iteration 373, loss = 0.07475662\n",
      "Iteration 374, loss = 0.07445234\n",
      "Iteration 375, loss = 0.07417577\n",
      "Iteration 376, loss = 0.07387402\n",
      "Iteration 377, loss = 0.07357060\n",
      "Iteration 378, loss = 0.07328466\n",
      "Iteration 379, loss = 0.07300057\n",
      "Iteration 380, loss = 0.07270342\n",
      "Iteration 381, loss = 0.07242865\n",
      "Iteration 382, loss = 0.07213478\n",
      "Iteration 383, loss = 0.07186860\n",
      "Iteration 384, loss = 0.07157161\n",
      "Iteration 385, loss = 0.07127911\n",
      "Iteration 386, loss = 0.07100788\n",
      "Iteration 387, loss = 0.07073970\n",
      "Iteration 388, loss = 0.07046852\n",
      "Iteration 389, loss = 0.07018491\n",
      "Iteration 390, loss = 0.06990360\n",
      "Iteration 391, loss = 0.06962582\n",
      "Iteration 392, loss = 0.06939022\n",
      "Iteration 393, loss = 0.06908657\n",
      "Iteration 394, loss = 0.06883309\n",
      "Iteration 395, loss = 0.06859431\n",
      "Iteration 396, loss = 0.06828475\n",
      "Iteration 397, loss = 0.06800694\n",
      "Iteration 398, loss = 0.06776169\n",
      "Iteration 399, loss = 0.06750963\n",
      "Iteration 400, loss = 0.06722961\n",
      "Iteration 401, loss = 0.06695905\n",
      "Iteration 402, loss = 0.06673066\n",
      "Iteration 403, loss = 0.06645632\n",
      "Iteration 404, loss = 0.06619041\n",
      "Iteration 405, loss = 0.06594124\n",
      "Iteration 406, loss = 0.06568762\n",
      "Iteration 407, loss = 0.06542340\n",
      "Iteration 408, loss = 0.06517280\n",
      "Iteration 409, loss = 0.06492059\n",
      "Iteration 410, loss = 0.06467054\n",
      "Iteration 411, loss = 0.06441280\n",
      "Iteration 412, loss = 0.06416616\n",
      "Iteration 413, loss = 0.06392794\n",
      "Iteration 414, loss = 0.06368518\n",
      "Iteration 415, loss = 0.06343754\n",
      "Iteration 416, loss = 0.06317318\n",
      "Iteration 417, loss = 0.06293822\n",
      "Iteration 418, loss = 0.06271292\n",
      "Iteration 419, loss = 0.06244736\n",
      "Iteration 420, loss = 0.06220339\n",
      "Iteration 421, loss = 0.06196889\n",
      "Iteration 422, loss = 0.06172981\n",
      "Iteration 423, loss = 0.06148167\n",
      "Iteration 424, loss = 0.06125742\n",
      "Iteration 425, loss = 0.06103613\n",
      "Iteration 426, loss = 0.06078962\n",
      "Iteration 427, loss = 0.06054906\n",
      "Iteration 428, loss = 0.06033421\n",
      "Iteration 429, loss = 0.06010147\n",
      "Iteration 430, loss = 0.05985848\n",
      "Iteration 431, loss = 0.05963766\n",
      "Iteration 432, loss = 0.05941122\n",
      "Iteration 433, loss = 0.05919363\n",
      "Iteration 434, loss = 0.05895274\n",
      "Iteration 435, loss = 0.05872764\n",
      "Iteration 436, loss = 0.05850779\n",
      "Iteration 437, loss = 0.05831864\n",
      "Iteration 438, loss = 0.05807997\n",
      "Iteration 439, loss = 0.05785447\n",
      "Iteration 440, loss = 0.05762538\n",
      "Iteration 441, loss = 0.05741708\n",
      "Iteration 442, loss = 0.05719654\n",
      "Iteration 443, loss = 0.05699808\n",
      "Iteration 444, loss = 0.05678217\n",
      "Iteration 445, loss = 0.05656064\n",
      "Iteration 446, loss = 0.05634668\n",
      "Iteration 447, loss = 0.05613317\n",
      "Iteration 448, loss = 0.05592391\n",
      "Iteration 449, loss = 0.05571416\n",
      "Iteration 450, loss = 0.05551523\n",
      "Iteration 451, loss = 0.05530861\n",
      "Iteration 452, loss = 0.05509708\n",
      "Iteration 453, loss = 0.05490208\n",
      "Iteration 454, loss = 0.05469951\n",
      "Iteration 455, loss = 0.05450454\n",
      "Iteration 456, loss = 0.05428133\n",
      "Iteration 457, loss = 0.05409406\n",
      "Iteration 458, loss = 0.05388858\n",
      "Iteration 459, loss = 0.05368171\n",
      "Iteration 460, loss = 0.05349905\n",
      "Iteration 461, loss = 0.05328395\n",
      "Iteration 462, loss = 0.05309499\n",
      "Iteration 463, loss = 0.05291716\n",
      "Iteration 464, loss = 0.05269877\n",
      "Iteration 465, loss = 0.05250903\n",
      "Iteration 466, loss = 0.05230838\n",
      "Iteration 467, loss = 0.05213552\n",
      "Iteration 468, loss = 0.05195649\n",
      "Iteration 469, loss = 0.05178705\n",
      "Iteration 470, loss = 0.05155839\n",
      "Iteration 471, loss = 0.05136669\n",
      "Iteration 472, loss = 0.05118299\n",
      "Iteration 473, loss = 0.05099254\n",
      "Iteration 474, loss = 0.05079711\n",
      "Iteration 475, loss = 0.05061238\n",
      "Iteration 476, loss = 0.05041689\n",
      "Iteration 477, loss = 0.05024365\n",
      "Iteration 478, loss = 0.05006598\n",
      "Iteration 479, loss = 0.04988005\n",
      "Iteration 480, loss = 0.04969956\n",
      "Iteration 481, loss = 0.04952813\n",
      "Iteration 482, loss = 0.04934764\n",
      "Iteration 483, loss = 0.04916300\n",
      "Iteration 484, loss = 0.04898830\n",
      "Iteration 485, loss = 0.04880436\n",
      "Iteration 486, loss = 0.04862541\n",
      "Iteration 487, loss = 0.04844979\n",
      "Iteration 488, loss = 0.04828041\n",
      "Iteration 489, loss = 0.04811548\n",
      "Iteration 490, loss = 0.04792958\n",
      "Iteration 491, loss = 0.04775251\n",
      "Iteration 492, loss = 0.04760163\n",
      "Iteration 493, loss = 0.04742090\n",
      "Iteration 494, loss = 0.04725676\n",
      "Iteration 495, loss = 0.04707142\n",
      "Iteration 496, loss = 0.04691275\n",
      "Iteration 497, loss = 0.04674622\n",
      "Iteration 498, loss = 0.04658334\n",
      "Iteration 499, loss = 0.04641436\n",
      "Iteration 500, loss = 0.04623687\n",
      "Iteration 501, loss = 0.04609316\n",
      "Iteration 502, loss = 0.04592331\n",
      "Iteration 503, loss = 0.04575324\n",
      "Iteration 504, loss = 0.04558157\n",
      "Iteration 505, loss = 0.04543122\n",
      "Iteration 506, loss = 0.04525486\n",
      "Iteration 507, loss = 0.04509798\n",
      "Iteration 508, loss = 0.04493812\n",
      "Iteration 509, loss = 0.04477751\n",
      "Iteration 510, loss = 0.04464644\n",
      "Iteration 511, loss = 0.04447727\n",
      "Iteration 512, loss = 0.04430253\n",
      "Iteration 513, loss = 0.04416083\n",
      "Iteration 514, loss = 0.04398873\n",
      "Iteration 515, loss = 0.04383258\n",
      "Iteration 516, loss = 0.04367440\n",
      "Iteration 517, loss = 0.04353917\n",
      "Iteration 518, loss = 0.04337409\n",
      "Iteration 519, loss = 0.04323611\n",
      "Iteration 520, loss = 0.04306743\n",
      "Iteration 521, loss = 0.04293940\n",
      "Iteration 522, loss = 0.04276648\n",
      "Iteration 523, loss = 0.04261056\n",
      "Iteration 524, loss = 0.04245999\n",
      "Iteration 525, loss = 0.04231975\n",
      "Iteration 526, loss = 0.04218215\n",
      "Iteration 527, loss = 0.04204068\n",
      "Iteration 528, loss = 0.04191537\n",
      "Iteration 529, loss = 0.04173235\n",
      "Iteration 530, loss = 0.04157268\n",
      "Iteration 531, loss = 0.04144315\n",
      "Iteration 532, loss = 0.04129778\n",
      "Iteration 533, loss = 0.04114332\n",
      "Iteration 534, loss = 0.04099705\n",
      "Iteration 535, loss = 0.04085758\n",
      "Iteration 536, loss = 0.04071203\n",
      "Iteration 537, loss = 0.04059174\n",
      "Iteration 538, loss = 0.04045557\n",
      "Iteration 539, loss = 0.04028761\n",
      "Iteration 540, loss = 0.04014936\n",
      "Iteration 541, loss = 0.04001196\n",
      "Iteration 542, loss = 0.03988753\n",
      "Iteration 543, loss = 0.03974490\n",
      "Iteration 544, loss = 0.03958854\n",
      "Iteration 545, loss = 0.03945563\n",
      "Iteration 546, loss = 0.03933473\n",
      "Iteration 547, loss = 0.03919086\n",
      "Iteration 548, loss = 0.03905857\n",
      "Iteration 549, loss = 0.03890722\n",
      "Iteration 550, loss = 0.03878805\n",
      "Iteration 551, loss = 0.03866049\n",
      "Iteration 552, loss = 0.03852176\n",
      "Iteration 553, loss = 0.03837499\n",
      "Iteration 554, loss = 0.03824611\n",
      "Iteration 555, loss = 0.03814639\n",
      "Iteration 556, loss = 0.03802106\n",
      "Iteration 557, loss = 0.03784449\n",
      "Iteration 558, loss = 0.03771533\n",
      "Iteration 559, loss = 0.03759206\n",
      "Iteration 560, loss = 0.03747320\n",
      "Iteration 561, loss = 0.03732715\n",
      "Iteration 562, loss = 0.03723030\n",
      "Iteration 563, loss = 0.03707831\n",
      "Iteration 564, loss = 0.03694868\n",
      "Iteration 565, loss = 0.03683239\n",
      "Iteration 566, loss = 0.03668964\n",
      "Iteration 567, loss = 0.03656884\n",
      "Iteration 568, loss = 0.03645109\n",
      "Iteration 569, loss = 0.03631341\n",
      "Iteration 570, loss = 0.03619166\n",
      "Iteration 571, loss = 0.03606302\n",
      "Iteration 572, loss = 0.03594589\n",
      "Iteration 573, loss = 0.03583378\n",
      "Iteration 574, loss = 0.03570455\n",
      "Iteration 575, loss = 0.03557757\n",
      "Iteration 576, loss = 0.03546068\n",
      "Iteration 577, loss = 0.03533796\n",
      "Iteration 578, loss = 0.03521007\n",
      "Iteration 579, loss = 0.03510456\n",
      "Iteration 580, loss = 0.03499378\n",
      "Iteration 581, loss = 0.03487303\n",
      "Iteration 582, loss = 0.03475360\n",
      "Iteration 583, loss = 0.03463795\n",
      "Iteration 584, loss = 0.03450837\n",
      "Iteration 585, loss = 0.03438322\n",
      "Iteration 586, loss = 0.03426257\n",
      "Iteration 587, loss = 0.03415246\n",
      "Iteration 588, loss = 0.03403310\n",
      "Iteration 589, loss = 0.03393333\n",
      "Iteration 590, loss = 0.03381339\n",
      "Iteration 591, loss = 0.03368398\n",
      "Iteration 592, loss = 0.03357414\n",
      "Iteration 593, loss = 0.03346087\n",
      "Iteration 594, loss = 0.03334182\n",
      "Iteration 595, loss = 0.03323389\n",
      "Iteration 596, loss = 0.03315965\n",
      "Iteration 597, loss = 0.03302163\n",
      "Iteration 598, loss = 0.03290851\n",
      "Iteration 599, loss = 0.03282762\n",
      "Iteration 600, loss = 0.03270374\n",
      "Iteration 601, loss = 0.03256824\n",
      "Iteration 602, loss = 0.03245967\n",
      "Iteration 603, loss = 0.03234489\n",
      "Iteration 604, loss = 0.03224973\n",
      "Iteration 605, loss = 0.03212332\n",
      "Iteration 606, loss = 0.03205495\n",
      "Iteration 607, loss = 0.03190582\n",
      "Iteration 608, loss = 0.03180519\n",
      "Iteration 609, loss = 0.03169209\n",
      "Iteration 610, loss = 0.03158635\n",
      "Iteration 611, loss = 0.03147838\n",
      "Iteration 612, loss = 0.03138301\n",
      "Iteration 613, loss = 0.03126539\n",
      "Iteration 614, loss = 0.03116726\n",
      "Iteration 615, loss = 0.03105391\n",
      "Iteration 616, loss = 0.03097320\n",
      "Iteration 617, loss = 0.03083533\n",
      "Iteration 618, loss = 0.03074017\n",
      "Iteration 619, loss = 0.03065219\n",
      "Iteration 620, loss = 0.03052216\n",
      "Iteration 621, loss = 0.03043280\n",
      "Iteration 622, loss = 0.03032282\n",
      "Iteration 623, loss = 0.03022236\n",
      "Iteration 624, loss = 0.03010852\n",
      "Iteration 625, loss = 0.03001339\n",
      "Iteration 626, loss = 0.02995285\n",
      "Iteration 627, loss = 0.02980683\n",
      "Iteration 628, loss = 0.02971098\n",
      "Iteration 629, loss = 0.02961398\n",
      "Iteration 630, loss = 0.02952520\n",
      "Iteration 631, loss = 0.02941940\n",
      "Iteration 632, loss = 0.02930600\n",
      "Iteration 633, loss = 0.02921104\n",
      "Iteration 634, loss = 0.02912617\n",
      "Iteration 635, loss = 0.02902612\n",
      "Iteration 636, loss = 0.02892527\n",
      "Iteration 637, loss = 0.02882005\n",
      "Iteration 638, loss = 0.02873255\n",
      "Iteration 639, loss = 0.02864743\n",
      "Iteration 640, loss = 0.02853098\n",
      "Iteration 641, loss = 0.02844216\n",
      "Iteration 642, loss = 0.02834680\n",
      "Iteration 643, loss = 0.02825845\n",
      "Iteration 644, loss = 0.02816560\n",
      "Iteration 645, loss = 0.02806318\n",
      "Iteration 646, loss = 0.02798683\n",
      "Iteration 647, loss = 0.02787140\n",
      "Iteration 648, loss = 0.02778518\n",
      "Iteration 649, loss = 0.02768987\n",
      "Iteration 650, loss = 0.02760089\n",
      "Iteration 651, loss = 0.02750230\n",
      "Iteration 652, loss = 0.02740888\n",
      "Iteration 653, loss = 0.02731628\n",
      "Iteration 654, loss = 0.02723534\n",
      "Iteration 655, loss = 0.02712674\n",
      "Iteration 656, loss = 0.02704949\n",
      "Iteration 657, loss = 0.02697921\n",
      "Iteration 658, loss = 0.02687376\n",
      "Iteration 659, loss = 0.02678863\n",
      "Iteration 660, loss = 0.02670606\n",
      "Iteration 661, loss = 0.02659939\n",
      "Iteration 662, loss = 0.02652596\n",
      "Iteration 663, loss = 0.02642730\n",
      "Iteration 664, loss = 0.02634025\n",
      "Iteration 665, loss = 0.02626174\n",
      "Iteration 666, loss = 0.02617348\n",
      "Iteration 667, loss = 0.02608048\n",
      "Iteration 668, loss = 0.02600431\n",
      "Iteration 669, loss = 0.02592399\n",
      "Iteration 670, loss = 0.02587826\n",
      "Iteration 671, loss = 0.02573866\n",
      "Iteration 672, loss = 0.02565019\n",
      "Iteration 673, loss = 0.02558369\n",
      "Iteration 674, loss = 0.02549395\n",
      "Iteration 675, loss = 0.02539459\n",
      "Iteration 676, loss = 0.02531289\n",
      "Iteration 677, loss = 0.02523788\n",
      "Iteration 678, loss = 0.02515019\n",
      "Iteration 679, loss = 0.02506370\n",
      "Iteration 680, loss = 0.02500411\n",
      "Iteration 681, loss = 0.02490655\n",
      "Iteration 682, loss = 0.02483837\n",
      "Iteration 683, loss = 0.02475274\n",
      "Iteration 684, loss = 0.02464542\n",
      "Iteration 685, loss = 0.02456987\n",
      "Iteration 686, loss = 0.02449499\n",
      "Iteration 687, loss = 0.02444930\n",
      "Iteration 688, loss = 0.02433213\n",
      "Iteration 689, loss = 0.02425327\n",
      "Iteration 690, loss = 0.02417398\n",
      "Iteration 691, loss = 0.02411367\n",
      "Iteration 692, loss = 0.02401399\n",
      "Iteration 693, loss = 0.02392851\n",
      "Iteration 694, loss = 0.02385717\n",
      "Iteration 695, loss = 0.02380747\n",
      "Iteration 696, loss = 0.02373037\n",
      "Iteration 697, loss = 0.02361161\n",
      "Iteration 698, loss = 0.02355026\n",
      "Iteration 699, loss = 0.02346078\n",
      "Iteration 700, loss = 0.02341524\n",
      "Iteration 701, loss = 0.02332437\n",
      "Iteration 702, loss = 0.02324987\n",
      "Iteration 703, loss = 0.02315986\n",
      "Iteration 704, loss = 0.02308829\n",
      "Iteration 705, loss = 0.02300509\n",
      "Iteration 706, loss = 0.02293377\n",
      "Iteration 707, loss = 0.02284950\n",
      "Iteration 708, loss = 0.02279663\n",
      "Iteration 709, loss = 0.02272471\n",
      "Iteration 710, loss = 0.02264388\n",
      "Iteration 711, loss = 0.02258852\n",
      "Iteration 712, loss = 0.02248502\n",
      "Iteration 713, loss = 0.02241323\n",
      "Iteration 714, loss = 0.02233516\n",
      "Iteration 715, loss = 0.02227445\n",
      "Iteration 716, loss = 0.02219326\n",
      "Iteration 717, loss = 0.02211245\n",
      "Iteration 718, loss = 0.02206067\n",
      "Iteration 719, loss = 0.02197414\n",
      "Iteration 720, loss = 0.02192136\n",
      "Iteration 721, loss = 0.02184784\n",
      "Iteration 722, loss = 0.02176509\n",
      "Iteration 723, loss = 0.02169648\n",
      "Iteration 724, loss = 0.02161746\n",
      "Iteration 725, loss = 0.02155186\n",
      "Iteration 726, loss = 0.02149777\n",
      "Iteration 727, loss = 0.02142056\n",
      "Iteration 728, loss = 0.02134528\n",
      "Iteration 729, loss = 0.02127089\n",
      "Iteration 730, loss = 0.02119813\n",
      "Iteration 731, loss = 0.02113454\n",
      "Iteration 732, loss = 0.02108000\n",
      "Iteration 733, loss = 0.02100700\n",
      "Iteration 734, loss = 0.02094797\n",
      "Iteration 735, loss = 0.02088537\n",
      "Iteration 736, loss = 0.02080606\n",
      "Iteration 737, loss = 0.02073335\n",
      "Iteration 738, loss = 0.02065924\n",
      "Iteration 739, loss = 0.02060770\n",
      "Iteration 740, loss = 0.02052208\n",
      "Iteration 741, loss = 0.02046765\n",
      "Iteration 742, loss = 0.02040313\n",
      "Iteration 743, loss = 0.02031745\n",
      "Iteration 744, loss = 0.02026210\n",
      "Iteration 745, loss = 0.02018436\n",
      "Iteration 746, loss = 0.02013188\n",
      "Iteration 747, loss = 0.02007460\n",
      "Iteration 748, loss = 0.01998757\n",
      "Iteration 749, loss = 0.01993429\n",
      "Iteration 750, loss = 0.01985322\n",
      "Iteration 751, loss = 0.01978418\n",
      "Iteration 752, loss = 0.01974177\n",
      "Iteration 753, loss = 0.01965786\n",
      "Iteration 754, loss = 0.01959256\n",
      "Iteration 755, loss = 0.01951827\n",
      "Iteration 756, loss = 0.01945574\n",
      "Iteration 757, loss = 0.01939053\n",
      "Iteration 758, loss = 0.01932573\n",
      "Iteration 759, loss = 0.01926696\n",
      "Iteration 760, loss = 0.01919709\n",
      "Iteration 761, loss = 0.01915836\n",
      "Iteration 762, loss = 0.01906853\n",
      "Iteration 763, loss = 0.01901463\n",
      "Iteration 764, loss = 0.01893682\n",
      "Iteration 765, loss = 0.01887733\n",
      "Iteration 766, loss = 0.01880758\n",
      "Iteration 767, loss = 0.01874504\n",
      "Iteration 768, loss = 0.01868007\n",
      "Iteration 769, loss = 0.01863443\n",
      "Iteration 770, loss = 0.01855144\n",
      "Iteration 771, loss = 0.01850134\n",
      "Iteration 772, loss = 0.01843655\n",
      "Iteration 773, loss = 0.01837500\n",
      "Iteration 774, loss = 0.01830533\n",
      "Iteration 775, loss = 0.01825960\n",
      "Iteration 776, loss = 0.01817502\n",
      "Iteration 777, loss = 0.01811935\n",
      "Iteration 778, loss = 0.01806185\n",
      "Iteration 779, loss = 0.01799581\n",
      "Iteration 780, loss = 0.01795014\n",
      "Iteration 781, loss = 0.01788240\n",
      "Iteration 782, loss = 0.01783068\n",
      "Iteration 783, loss = 0.01777134\n",
      "Iteration 784, loss = 0.01769762\n",
      "Iteration 785, loss = 0.01764586\n",
      "Iteration 786, loss = 0.01760566\n",
      "Iteration 787, loss = 0.01753689\n",
      "Iteration 788, loss = 0.01746768\n",
      "Iteration 789, loss = 0.01740598\n",
      "Iteration 790, loss = 0.01736263\n",
      "Iteration 791, loss = 0.01728995\n",
      "Iteration 792, loss = 0.01724568\n",
      "Iteration 793, loss = 0.01719433\n",
      "Iteration 794, loss = 0.01711115\n",
      "Iteration 795, loss = 0.01706467\n",
      "Iteration 796, loss = 0.01701526\n",
      "Iteration 797, loss = 0.01696601\n",
      "Iteration 798, loss = 0.01689436\n",
      "Iteration 799, loss = 0.01685010\n",
      "Iteration 800, loss = 0.01680001\n",
      "Iteration 801, loss = 0.01672986\n",
      "Iteration 802, loss = 0.01667359\n",
      "Iteration 803, loss = 0.01661085\n",
      "Iteration 804, loss = 0.01656335\n",
      "Iteration 805, loss = 0.01650741\n",
      "Iteration 806, loss = 0.01644739\n",
      "Iteration 807, loss = 0.01638750\n",
      "Iteration 808, loss = 0.01633823\n",
      "Iteration 809, loss = 0.01628671\n",
      "Iteration 810, loss = 0.01621984\n",
      "Iteration 811, loss = 0.01617173\n",
      "Iteration 812, loss = 0.01611291\n",
      "Iteration 813, loss = 0.01607441\n",
      "Iteration 814, loss = 0.01600351\n",
      "Iteration 815, loss = 0.01595773\n",
      "Iteration 816, loss = 0.01590576\n",
      "Iteration 817, loss = 0.01585266\n",
      "Iteration 818, loss = 0.01579945\n",
      "Iteration 819, loss = 0.01577308\n",
      "Iteration 820, loss = 0.01571316\n",
      "Iteration 821, loss = 0.01564621\n",
      "Iteration 822, loss = 0.01560005\n",
      "Iteration 823, loss = 0.01554904\n",
      "Iteration 824, loss = 0.01550493\n",
      "Iteration 825, loss = 0.01545281\n",
      "Iteration 826, loss = 0.01539977\n",
      "Iteration 827, loss = 0.01533793\n",
      "Iteration 828, loss = 0.01528470\n",
      "Iteration 829, loss = 0.01524016\n",
      "Iteration 830, loss = 0.01518431\n",
      "Iteration 831, loss = 0.01515419\n",
      "Iteration 832, loss = 0.01508391\n",
      "Iteration 833, loss = 0.01503234\n",
      "Iteration 834, loss = 0.01499026\n",
      "Iteration 835, loss = 0.01494342\n",
      "Iteration 836, loss = 0.01490264\n",
      "Iteration 837, loss = 0.01484590\n",
      "Iteration 838, loss = 0.01478309\n",
      "Iteration 839, loss = 0.01477804\n",
      "Iteration 840, loss = 0.01468890\n",
      "Iteration 841, loss = 0.01467083\n",
      "Iteration 842, loss = 0.01462955\n",
      "Iteration 843, loss = 0.01456274\n",
      "Iteration 844, loss = 0.01450729\n",
      "Iteration 845, loss = 0.01446299\n",
      "Iteration 846, loss = 0.01442127\n",
      "Iteration 847, loss = 0.01435452\n",
      "Iteration 848, loss = 0.01431938\n",
      "Iteration 849, loss = 0.01427312\n",
      "Iteration 850, loss = 0.01421345\n",
      "Iteration 851, loss = 0.01420055\n",
      "Iteration 852, loss = 0.01414136\n",
      "Iteration 853, loss = 0.01409722\n",
      "Iteration 854, loss = 0.01403214\n",
      "Iteration 855, loss = 0.01399224\n",
      "Iteration 856, loss = 0.01394576\n",
      "Iteration 857, loss = 0.01390545\n",
      "Iteration 858, loss = 0.01389759\n",
      "Iteration 859, loss = 0.01383162\n",
      "Iteration 860, loss = 0.01377692\n",
      "Iteration 861, loss = 0.01373539\n",
      "Iteration 862, loss = 0.01369764\n",
      "Iteration 863, loss = 0.01364549\n",
      "Iteration 864, loss = 0.01360543\n",
      "Iteration 865, loss = 0.01355027\n",
      "Iteration 866, loss = 0.01353113\n",
      "Iteration 867, loss = 0.01348239\n",
      "Iteration 868, loss = 0.01343544\n",
      "Iteration 869, loss = 0.01339171\n",
      "Iteration 870, loss = 0.01335647\n",
      "Iteration 871, loss = 0.01332435\n",
      "Iteration 872, loss = 0.01327370\n",
      "Iteration 873, loss = 0.01322202\n",
      "Iteration 874, loss = 0.01319205\n",
      "Iteration 875, loss = 0.01314697\n",
      "Iteration 876, loss = 0.01310448\n",
      "Iteration 877, loss = 0.01306987\n",
      "Iteration 878, loss = 0.01302292\n",
      "Iteration 879, loss = 0.01297538\n",
      "Iteration 880, loss = 0.01294368\n",
      "Iteration 881, loss = 0.01289743\n",
      "Iteration 882, loss = 0.01285251\n",
      "Iteration 883, loss = 0.01280585\n",
      "Iteration 884, loss = 0.01276881\n",
      "Iteration 885, loss = 0.01272389\n",
      "Iteration 886, loss = 0.01269064\n",
      "Iteration 887, loss = 0.01265403\n",
      "Iteration 888, loss = 0.01261879\n",
      "Iteration 889, loss = 0.01257442\n",
      "Iteration 890, loss = 0.01254103\n",
      "Iteration 891, loss = 0.01252596\n",
      "Iteration 892, loss = 0.01246036\n",
      "Iteration 893, loss = 0.01243324\n",
      "Iteration 894, loss = 0.01240616\n",
      "Iteration 895, loss = 0.01233985\n",
      "Iteration 896, loss = 0.01231032\n",
      "Iteration 897, loss = 0.01227583\n",
      "Iteration 898, loss = 0.01222183\n",
      "Iteration 899, loss = 0.01219448\n",
      "Iteration 900, loss = 0.01217478\n",
      "Iteration 901, loss = 0.01215404\n",
      "Iteration 902, loss = 0.01207152\n",
      "Iteration 903, loss = 0.01204345\n",
      "Iteration 904, loss = 0.01199740\n",
      "Iteration 905, loss = 0.01196353\n",
      "Iteration 906, loss = 0.01192541\n",
      "Iteration 907, loss = 0.01189626\n",
      "Iteration 908, loss = 0.01184592\n",
      "Iteration 909, loss = 0.01181354\n",
      "Iteration 910, loss = 0.01179472\n",
      "Iteration 911, loss = 0.01175848\n",
      "Iteration 912, loss = 0.01170650\n",
      "Iteration 913, loss = 0.01167275\n",
      "Iteration 914, loss = 0.01164306\n",
      "Iteration 915, loss = 0.01161779\n",
      "Iteration 916, loss = 0.01155916\n",
      "Iteration 917, loss = 0.01152458\n",
      "Iteration 918, loss = 0.01148006\n",
      "Iteration 919, loss = 0.01146098\n",
      "Iteration 920, loss = 0.01142686\n",
      "Iteration 921, loss = 0.01139730\n",
      "Iteration 922, loss = 0.01134391\n",
      "Iteration 923, loss = 0.01131916\n",
      "Iteration 924, loss = 0.01127132\n",
      "Iteration 925, loss = 0.01127029\n",
      "Iteration 926, loss = 0.01120685\n",
      "Iteration 927, loss = 0.01118457\n",
      "Iteration 928, loss = 0.01113721\n",
      "Iteration 929, loss = 0.01112200\n",
      "Iteration 930, loss = 0.01106319\n",
      "Iteration 931, loss = 0.01103485\n",
      "Iteration 932, loss = 0.01104144\n",
      "Iteration 933, loss = 0.01099283\n",
      "Iteration 934, loss = 0.01095221\n",
      "Iteration 935, loss = 0.01092643\n",
      "Iteration 936, loss = 0.01086004\n",
      "Iteration 937, loss = 0.01082942\n",
      "Iteration 938, loss = 0.01080613\n",
      "Iteration 939, loss = 0.01077918\n",
      "Iteration 940, loss = 0.01073694\n",
      "Iteration 941, loss = 0.01071373\n",
      "Iteration 942, loss = 0.01067596\n",
      "Iteration 943, loss = 0.01063623\n",
      "Iteration 944, loss = 0.01060286\n",
      "Iteration 945, loss = 0.01057456\n",
      "Iteration 946, loss = 0.01054971\n",
      "Iteration 947, loss = 0.01050620\n",
      "Iteration 948, loss = 0.01046717\n",
      "Iteration 949, loss = 0.01044916\n",
      "Iteration 950, loss = 0.01041713\n",
      "Iteration 951, loss = 0.01039396\n",
      "Iteration 952, loss = 0.01034354\n",
      "Iteration 953, loss = 0.01031554\n",
      "Iteration 954, loss = 0.01030708\n",
      "Iteration 955, loss = 0.01026017\n",
      "Iteration 956, loss = 0.01024609\n",
      "Iteration 957, loss = 0.01018944\n",
      "Iteration 958, loss = 0.01016394\n",
      "Iteration 959, loss = 0.01014361\n",
      "Iteration 960, loss = 0.01010458\n",
      "Iteration 961, loss = 0.01008541\n",
      "Iteration 962, loss = 0.01006786\n",
      "Iteration 963, loss = 0.01001117\n",
      "Iteration 964, loss = 0.01000541\n",
      "Iteration 965, loss = 0.00995176\n",
      "Iteration 966, loss = 0.00991567\n",
      "Iteration 967, loss = 0.00988282\n",
      "Iteration 968, loss = 0.00986500\n",
      "Iteration 969, loss = 0.00982562\n",
      "Iteration 970, loss = 0.00980153\n",
      "Iteration 971, loss = 0.00977187\n",
      "Iteration 972, loss = 0.00976693\n",
      "Iteration 973, loss = 0.00970752\n",
      "Iteration 974, loss = 0.00968639\n",
      "Iteration 975, loss = 0.00964609\n",
      "Iteration 976, loss = 0.00962506\n",
      "Iteration 977, loss = 0.00959373\n",
      "Iteration 978, loss = 0.00957176\n",
      "Iteration 979, loss = 0.00956398\n",
      "Iteration 980, loss = 0.00950180\n",
      "Iteration 981, loss = 0.00950376\n",
      "Iteration 982, loss = 0.00945274\n",
      "Iteration 983, loss = 0.00943245\n",
      "Iteration 984, loss = 0.00940391\n",
      "Iteration 985, loss = 0.00936494\n",
      "Iteration 986, loss = 0.00935633\n",
      "Iteration 987, loss = 0.00930937\n",
      "Iteration 988, loss = 0.00927811\n",
      "Iteration 989, loss = 0.00925915\n",
      "Iteration 990, loss = 0.00924145\n",
      "Iteration 991, loss = 0.00920076\n",
      "Iteration 992, loss = 0.00917525\n",
      "Iteration 993, loss = 0.00915177\n",
      "Iteration 994, loss = 0.00913397\n",
      "Iteration 995, loss = 0.00909159\n",
      "Iteration 996, loss = 0.00905881\n",
      "Iteration 997, loss = 0.00904497\n",
      "Iteration 998, loss = 0.00900435\n",
      "Iteration 999, loss = 0.00897831\n",
      "Iteration 1000, loss = 0.00895577\n",
      "Iteration 1001, loss = 0.00894953\n",
      "Iteration 1002, loss = 0.00889891\n",
      "Iteration 1003, loss = 0.00890036\n",
      "Iteration 1004, loss = 0.00885305\n",
      "Iteration 1005, loss = 0.00882519\n",
      "Iteration 1006, loss = 0.00879720\n",
      "Iteration 1007, loss = 0.00877194\n",
      "Iteration 1008, loss = 0.00875694\n",
      "Iteration 1009, loss = 0.00873026\n",
      "Iteration 1010, loss = 0.00869475\n",
      "Iteration 1011, loss = 0.00866062\n",
      "Iteration 1012, loss = 0.00862990\n",
      "Iteration 1013, loss = 0.00861968\n",
      "Iteration 1014, loss = 0.00858579\n",
      "Iteration 1015, loss = 0.00856994\n",
      "Iteration 1016, loss = 0.00854080\n",
      "Iteration 1017, loss = 0.00852507\n",
      "Iteration 1018, loss = 0.00848466\n",
      "Iteration 1019, loss = 0.00847844\n",
      "Iteration 1020, loss = 0.00844440\n",
      "Iteration 1021, loss = 0.00841700\n",
      "Iteration 1022, loss = 0.00840468\n",
      "Iteration 1023, loss = 0.00836517\n",
      "Iteration 1024, loss = 0.00835782\n",
      "Iteration 1025, loss = 0.00830898\n",
      "Iteration 1026, loss = 0.00828753\n",
      "Iteration 1027, loss = 0.00826717\n",
      "Iteration 1028, loss = 0.00824374\n",
      "Iteration 1029, loss = 0.00821855\n",
      "Iteration 1030, loss = 0.00820207\n",
      "Iteration 1031, loss = 0.00816797\n",
      "Iteration 1032, loss = 0.00815191\n",
      "Iteration 1033, loss = 0.00811888\n",
      "Iteration 1034, loss = 0.00809877\n",
      "Iteration 1035, loss = 0.00809583\n",
      "Iteration 1036, loss = 0.00805425\n",
      "Iteration 1037, loss = 0.00804611\n",
      "Iteration 1038, loss = 0.00801202\n",
      "Iteration 1039, loss = 0.00798957\n",
      "Iteration 1040, loss = 0.00798010\n",
      "Iteration 1041, loss = 0.00793714\n",
      "Iteration 1042, loss = 0.00791469\n",
      "Iteration 1043, loss = 0.00788915\n",
      "Iteration 1044, loss = 0.00786810\n",
      "Iteration 1045, loss = 0.00785269\n",
      "Iteration 1046, loss = 0.00783523\n",
      "Iteration 1047, loss = 0.00780272\n",
      "Iteration 1048, loss = 0.00776848\n",
      "Iteration 1049, loss = 0.00774990\n",
      "Iteration 1050, loss = 0.00775045\n",
      "Iteration 1051, loss = 0.00772662\n",
      "Iteration 1052, loss = 0.00772139\n",
      "Iteration 1053, loss = 0.00767561\n",
      "Iteration 1054, loss = 0.00764734\n",
      "Iteration 1055, loss = 0.00762040\n",
      "Iteration 1056, loss = 0.00761645\n",
      "Iteration 1057, loss = 0.00758729\n",
      "Iteration 1058, loss = 0.00755890\n",
      "Iteration 1059, loss = 0.00754830\n",
      "Iteration 1060, loss = 0.00753161\n",
      "Iteration 1061, loss = 0.00748477\n",
      "Iteration 1062, loss = 0.00747558\n",
      "Iteration 1063, loss = 0.00744191\n",
      "Iteration 1064, loss = 0.00743339\n",
      "Iteration 1065, loss = 0.00741325\n",
      "Iteration 1066, loss = 0.00739016\n",
      "Iteration 1067, loss = 0.00738381\n",
      "Iteration 1068, loss = 0.00735047\n",
      "Iteration 1069, loss = 0.00732257\n",
      "Iteration 1070, loss = 0.00729610\n",
      "Iteration 1071, loss = 0.00727842\n",
      "Iteration 1072, loss = 0.00725634\n",
      "Iteration 1073, loss = 0.00723636\n",
      "Iteration 1074, loss = 0.00721751\n",
      "Iteration 1075, loss = 0.00725300\n",
      "Iteration 1076, loss = 0.00717397\n",
      "Iteration 1077, loss = 0.00716873\n",
      "Iteration 1078, loss = 0.00713120\n",
      "Iteration 1079, loss = 0.00712237\n",
      "Iteration 1080, loss = 0.00709702\n",
      "Iteration 1081, loss = 0.00706491\n",
      "Iteration 1082, loss = 0.00706956\n",
      "Iteration 1083, loss = 0.00703510\n",
      "Iteration 1084, loss = 0.00700940\n",
      "Iteration 1085, loss = 0.00700146\n",
      "Iteration 1086, loss = 0.00698022\n",
      "Iteration 1087, loss = 0.00696883\n",
      "Iteration 1088, loss = 0.00694757\n",
      "Iteration 1089, loss = 0.00693077\n",
      "Iteration 1090, loss = 0.00690152\n",
      "Iteration 1091, loss = 0.00687379\n",
      "Iteration 1092, loss = 0.00685338\n",
      "Iteration 1093, loss = 0.00683416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppmmr\\Documents\\TRABALHO-FINLANDIA\\Date Science\\Notebook ML\\ML\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1093) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1093, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1093, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1093, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRedeNeural = MLPClassifier(max_iter=1093, verbose=True, tol=0.0000100, solver='adam', activation='relu', hidden_layer_sizes=(2,2))\n",
    "\n",
    "creditRedeNeural.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = creditRedeNeural.predict(X_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "cm = ConfusionMatrix(creditRedeNeural)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       348\n",
      "           1       0.98      1.00      0.99        52\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       0.99      1.00      0.99       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f55f260a693975c2955a3b52cedb65e0d37bfa0d66d90c27450f86ae77a77bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
